{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "number_of_features = 1000\n",
    "\n",
    "# Load data and target vector from movie review data\n",
    "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# Use padding or truncation to make each observation have 400 features\n",
    "train_features = sequence.pad_sequences(train_data, maxlen=400)\n",
    "test_features = sequence.pad_sequences(test_data, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         1,  14,  22,  16,  43, 530, 973,   2,   2,  65, 458,   2,  66,\n",
       "         2,   4, 173,  36, 256,   5,  25, 100,  43, 838, 112,  50, 670,\n",
       "         2,   9,  35, 480, 284,   5, 150,   4, 172, 112, 167,   2, 336,\n",
       "       385,  39,   4, 172,   2,   2,  17, 546,  38,  13, 447,   4, 192,\n",
       "        50,  16,   6, 147,   2,  19,  14,  22,   4,   2,   2, 469,   4,\n",
       "        22,  71,  87,  12,  16,  43, 530,  38,  76,  15,  13,   2,   4,\n",
       "        22,  17, 515,  17,  12,  16, 626,  18,   2,   5,  62, 386,  12,\n",
       "         8, 316,   8, 106,   5,   4,   2,   2,  16, 480,  66,   2,  33,\n",
       "         4, 130,  12,  16,  38, 619,   5,  25, 124,  51,  36, 135,  48,\n",
       "        25,   2,  33,   6,  22,  12, 215,  28,  77,  52,   5,  14, 407,\n",
       "        16,  82,   2,   8,   4, 107, 117,   2,  15, 256,   4,   2,   7,\n",
       "         2,   5, 723,  36,  71,  43, 530, 476,  26, 400, 317,  46,   7,\n",
       "         4,   2,   2,  13, 104,  88,   4, 381,  15, 297,  98,  32,   2,\n",
       "        56,  26, 141,   6, 194,   2,  18,   4, 226,  22,  21, 134, 476,\n",
       "        26, 480,   5, 144,  30,   2,  18,  51,  36,  28, 224,  92,  25,\n",
       "       104,   4, 226,  65,  16,  38,   2,  88,  12,  16, 283,   5,  16,\n",
       "         2, 113, 103,  32,  15,  16,   2,  19, 178,  32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
    "\n",
    "# Add a long short-term memory layer with 128 units\n",
    "network.add(layers.LSTM(units=128))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network\n",
    "network.compile(loss='binary_crossentropy', # Cross-entropy\n",
    "                optimizer='Adam', # Adam optimization\n",
    "                metrics=['accuracy']) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-69ca6ed039d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Do not print description after each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Number of observations per batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                       validation_data=(test_features, test_target)) # Data for evaluation\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = network.fit(train_features, # Features\n",
    "                      train_target, # Target\n",
    "                      epochs=3, # Number of epochs\n",
    "                      verbose=0, # Do not print description after each epoch\n",
    "                      batch_size=1000, # Number of observations per batch\n",
    "                      validation_data=(test_features, test_target)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing import sequence\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing import sequence\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "# Load libraries\n",
      "import numpy as np\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing import sequence\n",
      "from keras import models\n",
      "from keras import layers\n",
      "\n",
      "# Set random seed\n",
      "np.random.seed(0)\n",
      "number_of_features = 1000\n",
      "\n",
      "# Load data and target vector from movie review data\n",
      "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)\n",
      "\n",
      "# Use padding or truncation to make each observation have 400 features\n",
      "train_features = sequence.pad_sequences(train_data, maxlen=400)\n",
      "test_features = sequence.pad_sequences(test_data, maxlen=400)\n",
      "train_data\n",
      "train_features\n",
      "train_features.shape\n",
      "train_data.shape\n",
      "train_data\n",
      "train_data[0]\n",
      "len(train_data[0])\n",
      "len(train_features[0])\n",
      "len(train_features)\n",
      "train_features\n",
      "train_features[0]\n",
      "train_features[0]\n",
      "# Start neural network\n",
      "network = models.Sequential()\n",
      "\n",
      "# Add an embedding layer\n",
      "network.add(layers.Embedding(input_dim=number_of_features, output_dim=128))\n",
      "\n",
      "# Add a long short-term memory layer with 128 units\n",
      "network.add(layers.LSTM(units=128))\n",
      "\n",
      "# Add fully connected layer with a sigmoid activation function\n",
      "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
      "# Compile neural network\n",
      "network.compile(loss='binary_crossentropy', # Cross-entropy\n",
      "                optimizer='Adam', # Adam optimization\n",
      "                metrics=['accuracy']) # Accuracy performance metric\n",
      "# Train neural network\n",
      "history = network.fit(train_features, # Features\n",
      "                      train_target, # Target\n",
      "                      epochs=3, # Number of epochs\n",
      "                      verbose=0, # Do not print description after each epoch\n",
      "                      batch_size=1000, # Number of observations per batch\n",
      "                      validation_data=(test_features, test_target)) # Data for evaluation\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
